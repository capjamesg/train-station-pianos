# This is a robots.txt file. It is a file that many popular search engines and some AI bots read to decide
# what they will "crawl". Crawling involves downloading, programmatically reading, and, often, saving
# data for future use. Search engines crawl web pages then "index" them. Indexing involves saving a web page
# so it can be returned in search results.

# The robots.txt file below excludes many AI bots that promise to adhere to the robots.txt protocol,
# among several other bots.

# I am content with Google indexing my content to show in search: to the extent my site can help someone answer a question,
# or satisfy their curiosity, I am glad! But I don't want my words used to train LLMs.

User-agent: ia_archiver
Disallow: /

User-agent: GPTBot
Disallow: /

User-agent: ChatGPT-User
Disallow: /

User-agent: Google-Extended
Disallow: /

User-agent: CCBot
Disallow: /

User-agent: FacebookBot
Disallow: /

User-agent: Twitterbot
Disallow: /

User-agent: Googlebot-Image
Disallow: /

User-agent: Pinterestbot
Disallow: /

User-agent: Amazonbot
Disallow: /

User-agent: anthropic-ai
Disallow: /

User-agent: AwarioRssBot
User-agent: AwarioSmartBot
Disallow: /

User-agent: Bytespider
Disallow: /

User-agent: CCBot
Disallow: /

User-agent: ChatGPT-User
Disallow: /

User-agent: ClaudeBot
Disallow: /

User-agent: Claude-Web
Disallow: /

User-agent: cohere-ai
Disallow: /

User-agent: DataForSeoBot
Disallow: /

User-agent: FacebookBot
Disallow: /

User-agent: Google-Extended
Disallow: /

User-agent: GPTBot
Disallow: /

User-agent: magpie-crawler
Disallow: /

User-agent: omgili
Disallow: /

User-agent: omgilibot
Disallow: /

User-agent: peer39_crawler
User-agent: peer39_crawler/1.0
Disallow: /

User-agent: PerplexityBot
Disallow: /

User-agent: *
Disallow: /s/

Sitemap: /sitemap.xml